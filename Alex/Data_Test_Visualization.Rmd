---
title: "HousingML_Visualisation"
author: "Alex Tin"
date: "5/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)



```

Import files

```{r}

iowatrain = read.csv('/Users/alexandertin/Desktop/HousingML/data/train.csv', row.names = 'Id')

train = iowatrain

iowatrim = read.csv('../datafilter/train05-23.csv', row.names = 'Id')

trimmed = iowatrim
```


Libraries
```{r}
#For missingness
library(VIM)
library(mice)
library(naniar)

#EDA
library(car)

library(MASS)
library(caret)

library(leaps)

library(dplyr)
```


Missingness Visualization (taken care of in Python)
```{r}
#Using naniar library, counting the number of features with missing variables
n_var_miss(iowatrain)

#Visualize connections with missing values
gg_miss_upset(iowatrain, nsets=n_var_miss(iowatrain))


gg_miss_which(iowatrain)



```



Features:
Continuous (21)
LotFrontage, LotArea, YearBuilt, YearRemodAdd, MasVnrArea, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageYrBlt, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, X3SsnPorch, ScreenPorch, MiscVal, YrSold, SalePrice, House_Age, Remod_Age, Garage_Age


Data manipulation
```{r}
#Convert MSSubClass to factor
trimmed$MSSubClass = as.factor(trimmed$MSSubClass)
trimmed$OverallQual = as.factor(trimmed$OverallQual)
trimmed$OverallCond = as.factor(trimmed$OverallCond)
trimmed$Alley = as.factor(trimmed$Alley)

#Take log of Sale Price, 
trimmed$SalePrice = log(trimmed$SalePrice)
trimmed$GrLivArea = log(trimmed$GrLivArea)
trimmed$LotArea = log(trimmed$LotArea)

summary(trimmed)
str(train)

eng.train = trimmed %>% mutate(., HouseAge = max(YearBuilt)-YearBuilt, TotBath = FullBath + HalfBath*0.5, LotFrontage = ifelse(is.na(LotFrontage), mean(LotFrontage, na.rm=TRUE),LotFrontage)) 

drops <- c('X', 'FullBath','HalfBath','YearBuilt')
eng.train = eng.train[ , !(names(eng.train) %in% drops)]



```


ANOVA tests

```{r}
factorlist = sapply(iowatrim, function(x) is.factor(x))

iowafactor <- iowatrim[,factorlist]

which(sapply(iowafactor, function(x) length(unique(x))<2))

bsmtlist = colnames(iowatrim[grep('Bsmt',colnames(iowatrim))])
bsmtlist





regfit.full=regsubsets(SalePrice ~., iowatrim, )




# Full model
housing = lm(SalePrice ~ . -LotFrontage, data=iowatrim)



#Model will all features except Basement
housing.nobsmt = lm(SalePrice ~ . - BsmtQual - BsmtCond - BsmtExposure - BsmtFinType1 - BsmtFinType2 - TotalBsmtSF - BsmtFullBath - BsmtHalfBath, data=iowatrim)


#Model with all features except Garage
housing.nogarage = 



```



Basic House - Lot Size, bedrooms, bathrooms, age, kitchen
```{r}

basic = eng.train %>% select(., LotArea, GrLivArea, BedroomAbvGr, KitchenAbvGr,HouseAge, TotBath, SalePrice)

```


Linear Regression of Basic House
```{r}
basic.model = lm(SalePrice~., data = basic)

summary(basic.model)
plot(basic.model)
influencePlot(basic.model)
vif(basic.model)
```

Regression Tree
```{r}
library(tree)

#Creating train set on 80% of data
set.seed(0)
train80 = sample(1:nrow(eng.train),8*nrow(eng.train)/10)

#Training tree
basic.tree = tree(SalePrice ~ ., eng.train, subset = train80)

#Results of train tree
summary(basic.tree)
plot(basic.tree)
text(basic.tree, pretty = 0)
basic.tree

#Prune tree
prune.basic = prune.tree(basic.tree)
par(mfrow = c(1,1))
plot(prune.basic)


#Caculate and asses MSE of test data on overall tree


```


Random Forest
```{r}
library(randomForest)

#Fitting random forest to train set
set.seed(0)
rf.train = randomForest(SalePrice ~ ., data = eng.train, subset = train80, na.action = na.exclude, ntree = 1000 , importance = TRUE)

rf.train

```

Vary the number of variables used at each step of random forest procedure
```{r}
set.seed(0)
oob.err=numeric(73)
for (mtry in 1:73){
  fit = randomForest(SalePrice ~ ., data=eng.train[train80, ], mtry=mtry, na.action = na.exclude)
  oob.err[mtry] = fit$mse[500]
  cat("We're performing interation", mtry, "\n")
}

```

```{r}
plot(1:78, oob.err, pch = 16, type = 'b',
     xlab = "Variables Considered at Each Split",
     ylab = "OOB Mean Squared Error",
     main = "Random Forest OOB Error Rates\nby # of Variables")

```

```{r}
importance(rf.train)
varImpPlot(rf.train)
```


Boosting
```{r}
library(gbm)

#`10000 trees
set.seed(0)
boost.train = gbm(SalePrice ~ ., data = eng.train[train80, ],
                  distribution = 'gaussian',
                  n.trees = 10000,
                  interaction.depth=4)

#Look at the relative influence
par(mfrow = c(1,1))
summary(boost.train)

# Making prediction on test set
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.train, newdata = eng.train[-train80, ], n.trees = n.trees)


dim(predmat)

#Calculate boosted errors
par(mfrow = c(1,1))
berr = with(eng.train[-train80, ], apply((predmat - SalePrice)^2,2,mean))
plot(n.trees, berr, pch=16,
     ylab = "Mean Squared Error",
     xlab = "# Trees",
     main = "Boosting Test Error")
abline(h=min(oob.err), col='red')
```


Experiment 1
```{r}
experiment1 = eng.train %>% select(., LotArea, GrLivArea, BedroomAbvGr, KitchenAbvGr,HouseAge, TotBath,  SalePrice, OverallQual)

exp1.model = lm(SalePrice~., data = experiment1)

summary(exp1.model)
plot(exp1.model)
influencePlot(exp1.model)
vif(exp1.model)


```

Partial f-test to compare 
```{r}
anova(basic.model, experiment2)
```



Experiment 2 (Tori's Selection)
```{r}
experiment2 = eng.train %>% select(., MSSubClass, MSZoning, LotArea, Alley, Utilities, Neighborhood, Condition1, BldgType, YearBuilt, YearRemodAdd, Exterior1st, TotalBsmtSF, Heating, CentralAir, Fireplaces, GarageType, GarageArea, GrLivArea, BedroomAbvGr, KitchenAbvGr,HouseAge, TotBath,  SalePrice, OverallQual)

exp2.model = lm(SalePrice~., data = experiment2)

summary(exp2.model)
plot(exp2.model)
influencePlot(exp2.model)
vif(exp2.model)


```



Random Forest

```{r}
# set.seed(0)
# rf.train1 = sample(1:nrow(trimmed), 8*nrow(trimmed)/10) # train indice
# rf.test1 = trimmed[-rf.train1, ] #test dataset
# response.test = trimmed[-rf.train1, 'SalePrice']


```














